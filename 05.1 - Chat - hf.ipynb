{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92a763-e30d-4bae-8a79-9cb0d8a58f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "## Quantized Gemma model, stored locally\n",
    "# model_id = \"models/google__gemma-2-27b-it\"\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "## Quantized Llama model, stored locally\n",
    "# model_id = \"models/meta-llama__Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"device_map\": \"sequential\", # load the model into GPUs sequentially, to avoid memory allocation issues with balancing\n",
    "    \"torch_dtype\": \"auto\"\n",
    "}\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e20b5c-3ff7-49ce-9db5-8c4fe13f487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d92f5e-942f-4d9f-a0c8-75c5883ee82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_model_info(pipe.model)\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ccc11-4f71-433c-a375-08a4c46cb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "if 'llama' in model_id.lower():\n",
    "    generate_kwargs[\"eos_token_id\"] = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "## Mistral states that lower temperatures should be used with Nemo\n",
    "if 'nemo' in model_id.lower():\n",
    "    generate_kwargs[\"temperature\"] = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7909c87-9ab9-405f-afce-d599071d71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are the user's friend and you care about their well being.\n",
    "You will converse with the user and attempt to get to know them better.\n",
    "You will only ask one question in each of your responses.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt,\n",
    "    }\n",
    "]\n",
    "\n",
    "_ = pipe(messages, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e67297-ad5e-44b9-9b3a-62a786ce195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Everything is good! I'm excited to be developing AI technology lately.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "_ = pipe(messages, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868079ec-4a61-4df7-ae2c-12346e9de4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I want to develop advanced tools for personalizing AI interactions.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "_ = pipe(messages, **generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa0b15-eec6-4a5c-bdd5-8730a8cc46c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
