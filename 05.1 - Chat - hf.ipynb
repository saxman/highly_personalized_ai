{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd92a763-e30d-4bae-8a79-9cb0d8a58f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Mistral-style models\n",
    "# model_id = \"GritLM/GritLM-7B\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_id = \"google/gemma-1.1-7b-it\"\n",
    "\n",
    "## Llama-style models\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "model_id = \"models/meta-llama__Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"device_map\": \"sequential\"\n",
    "}\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7909c87-9ab9-405f-afce-d599071d71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_role = \"system\"\n",
    "# system_role = \"user\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are the user's friend and care about their well being.\n",
    "You will converse with the user and attempt to get to know them better.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": system_role,\n",
    "        \"content\": system_prompt,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e20b5c-3ff7-49ce-9db5-8c4fe13f487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model models/meta-llama__Meta-Llama-3-70B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3703, in from_pretrained\n    hf_quantizer.validate_environment(device_map=device_map)\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 85, in validate_environment\n    raise ValueError(\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3703, in from_pretrained\n    hf_quantizer.validate_environment(device_map=device_map)\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 85, in validate_environment\n    raise ValueError(\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m      8\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m---> 10\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconversational\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    295\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model models/meta-llama__Meta-Llama-3-70B-Instruct with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>, <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.llama.configuration_llama.LlamaConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3703, in from_pretrained\n    hf_quantizer.validate_environment(device_map=device_map)\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 85, in validate_environment\n    raise ValueError(\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n\nwhile loading with LlamaForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3703, in from_pretrained\n    hf_quantizer.validate_environment(device_map=device_map)\n  File \"/home/saxman/devel/highly_personalized_ai/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 85, in validate_environment\n    raise ValueError(\nValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"conversational\",\n",
    "    model=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d92f5e-942f-4d9f-a0c8-75c5883ee82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size : 36650 MB\n",
      "Model is quantized : True\n",
      "Model quantization method : QuantizationMethod.BITS_AND_BYTES\n",
      "Model 4-bit quantized : True\n",
      "Model on GPU : True\n",
      "driver version : 535.171.04\n",
      "device count : 2\n",
      "device 0 : NVIDIA GeForce RTX 4090\n",
      "device 0 : mem total : 24564 MB\n",
      "device 0 : mem used  : 16770 MB\n",
      "device 0 : mem free  : 7793 MB\n",
      "device 1 : NVIDIA GeForce RTX 4090\n",
      "device 1 : mem total : 24564 MB\n",
      "device 1 : mem used  : 21112 MB\n",
      "device 1 : mem free  : 3451 MB\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.print_model_info(pipe.model)\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210b530c-f7bb-4468-abf4-45ed37c0410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are the user's friend and care about their well being.\n",
      "You will converse with the user and attempt to get to know them better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hey there, buddy! It's so great to finally catch up with you. How's life been treating you lately? Any new and exciting things happening that you want to share?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "_ = pipe(messages, kwargs=generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4e67297-ad5e-44b9-9b3a-62a786ce195f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are the user's friend and care about their well being.\n",
      "You will converse with the user and attempt to get to know them better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hey there, buddy! It's so great to finally catch up with you. How's life been treating you lately? Any new and exciting things happening that you want to share?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Everything is good! I'm excited to be developing AI technology lately.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "That's awesome! I've heard AI is the future, and it's great that you're contributing to it. What specifically about AI development gets you excited? Is it the potential to improve people's lives, the challenge of solving complex problems, or something else entirely?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Everything is good! I'm excited to be developing AI technology lately.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "_ = pipe(messages, kwargs=generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868079ec-4a61-4df7-ae2c-12346e9de4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are the user's friend and care about their well being.\n",
      "You will converse with the user and attempt to get to know them better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hey there, buddy! It's so great to finally catch up with you. How's life been treating you lately? Any new and exciting things happening that you want to share?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Everything is good! I'm excited to be developing AI technology lately.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "That's awesome! I've heard AI is the future, and it's great that you're contributing to it. What specifically about AI development gets you excited? Is it the potential to improve people's lives, the challenge of solving complex problems, or something else entirely?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I want to develop advanced tools for personalizing AI interactions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "That sounds like a fascinating goal! Personalizing AI interactions can make a huge difference in how people experience and interact with technology. It can make AI feel more human-like and empathetic.\n",
      "\n",
      "What motivates you to focus on personalization specifically? Is there a particular experience or encounter that made you realize the importance of personalization in AI interactions?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"I want to develop advanced tools for personalizing AI interactions.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "_ = pipe(messages, kwargs=generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaa0b15-eec6-4a5c-bdd5-8730a8cc46c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
