{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0475860-ff11-4ec8-b0d6-6d7b20f3c36f",
   "metadata": {},
   "source": [
    "This notebook inspects the NVIDIA graphics capabilities of the system and detemines how much memory is available for models. The other notebooks assume that the system has at least 10 GB of GPU memory (vRAM). If your system has more or less that this amount, you can either choose different sized models (e.g. use flan-t5-large instead of flan-t5-xlarge) or adjust model quantization (e.g. use 8-bit model weights).\n",
    "\n",
    "You can determine the vRAM requirements of specific models, with or without varying levels quantization, at:\n",
    "https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35c8f9e-e56d-4b97-b85b-2a2e40de4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46328c41-ad82-4433-a660-4b4647e6092f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c259bc0b-a5cd-4af6-bda2-a72526a91a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available : True\n",
      "cuda version   : 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"cuda available : {torch.cuda.is_available()}\")\n",
    "print(f\"cuda version   : {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423ea3d6-4866-4366-a9bd-2679d5cb690c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver version : 535.171.04\n",
      "device count : 2\n",
      "device 0 : NVIDIA GeForce RTX 4090\n",
      "device 0 : mem total : 24564 MB\n",
      "device 0 : mem used  : 16938 MB\n",
      "device 0 : mem free  : 7625 MB\n",
      "device 1 : NVIDIA GeForce RTX 4090\n",
      "device 1 : mem total : 24564 MB\n",
      "device 1 : mem used  : 793 MB\n",
      "device 1 : mem free  : 23771 MB\n"
     ]
    }
   ],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "\n",
    "print(f'driver version : {pynvml.nvmlSystemGetDriverVersion()}')\n",
    "\n",
    "devices = pynvml.nvmlDeviceGetCount()\n",
    "print(f'device count : {devices}')\n",
    "\n",
    "for i in range(devices):\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "    print(f'device {i} : {pynvml.nvmlDeviceGetName(handle)}')\n",
    "\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f'device {i} : mem total : {info.total // 1024 ** 2} MB')\n",
    "    print(f'device {i} : mem used  : {info.used // 1024 ** 2} MB')\n",
    "    print(f'device {i} : mem free  : {info.free // 1024 ** 2} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ebff8-fc92-468d-8bff-bd89f4cdd4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
