{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b792e89f-15b0-415b-b5ea-6a65177424a1",
   "metadata": {},
   "source": [
    "Required for progress meters and other notebook widgets required by the following packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945029c1-1d72-4153-b13a-1d7770ea0f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539f037-ad7a-43f2-9a15-f0a028f51f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d12c9-187f-4762-a2a2-77bbb92cb66b",
   "metadata": {},
   "source": [
    "Log into Huggingface for accessing gated models (e.g. Meta's Llama models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847b941-4db6-4e74-8a11-2b3f755174e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511f719-52de-4c76-91eb-d73b99e6060f",
   "metadata": {},
   "source": [
    "For models that we want to use in a quantized state (e.g. Llama 3 70B), compute and store quantized version of the models to reduce load times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e460f29-d0f8-4d78-9cd8-16d486a2a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64f347-fdab-4bf6-94a9-70753db289e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31de1f-e4b1-4076-89f8-f80010353404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "## 8-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True\n",
    "# )\n",
    "\n",
    "## 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    # device_map=\"auto\"\n",
    "    device_map=\"sequential\" ## using sequential instead of auto/balanced since otherwise lm_head gets put on CPU\n",
    ")\n",
    "\n",
    "quantized_model_id = \"models/\" + model_id.replace(\"/\", \"__\")\n",
    "\n",
    "model.save_pretrained(quantized_model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(quantized_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5836ff-0ed6-4875-93ca-5357756e4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37898818-6e78-485d-b7ec-f997a4810237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667d612-9a8f-46bb-92df-11bc78f0b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd2010-c4f8-496d-910f-b3ffcf87334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496ca1a-d0dd-4694-94d0-c35f1d9e6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "quantized_model_id = \"models/meta-llama__Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_id,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    quantized_model_id,\n",
    "    device_map=\"sequential\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833dfad-60fd-4b90-a571-3aac2aa6a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_model_info(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
