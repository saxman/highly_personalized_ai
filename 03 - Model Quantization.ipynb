{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e511f719-52de-4c76-91eb-d73b99e6060f",
   "metadata": {},
   "source": [
    "For models that we want to use in a quantized state (e.g. Llama 3 70B), compute and store quantized version of the models to reduce load times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e460f29-d0f8-4d78-9cd8-16d486a2a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769c44f-2f60-45ce-be2a-24d6a4dffdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64f347-fdab-4bf6-94a9-70753db289e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31de1f-e4b1-4076-89f8-f80010353404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "## 8-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True\n",
    "# )\n",
    "\n",
    "## 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"sequential\" ## using sequential instead of auto/balanced since otherwise lm_head gets put on CPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81964122-3679-4692-9b4a-e274fbc79ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"models/\" + model_id.replace(\"/\", \"__\")\n",
    "\n",
    "model.save_pretrained(quantized_model_dir)\n",
    "_ = tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5836ff-0ed6-4875-93ca-5357756e4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_map(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667d612-9a8f-46bb-92df-11bc78f0b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd2010-c4f8-496d-910f-b3ffcf87334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496ca1a-d0dd-4694-94d0-c35f1d9e6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    quantized_model_dir,\n",
    "    device_map=\"sequential\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833dfad-60fd-4b90-a571-3aac2aa6a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_map(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25fb12-0a14-43f4-9341-a3f52944e17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
