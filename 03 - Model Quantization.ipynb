{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e511f719-52de-4c76-91eb-d73b99e6060f",
   "metadata": {},
   "source": [
    "For models that we want to use in a quantized state (e.g. Llama 3 70B), compute and store quantized version of the models to reduce load times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e460f29-d0f8-4d78-9cd8-16d486a2a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769c44f-2f60-45ce-be2a-24d6a4dffdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64f347-fdab-4bf6-94a9-70753db289e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31de1f-e4b1-4076-89f8-f80010353404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "## 8-bit quantization\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "## 4-bit quantization\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config_8bit,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"sequential\" ## using sequential instead of auto/balanced since otherwise lm_head gets put on CPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81964122-3679-4692-9b4a-e274fbc79ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"models/\" + model_id.replace(\"/\", \"__\")\n",
    "\n",
    "model.save_pretrained(quantized_model_dir)\n",
    "_ = tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a0462-79b2-4688-9fdf-3a894d54f54b",
   "metadata": {},
   "source": [
    "If the model was reduced to a size that will fit into GPU memory, through quantization, when we print the model device map, the items listed should all map the GPU device (or one of the devices) listed by print_device_info. None of the layers should map to CPU. For example, if we have two GPU devices available and listed (0 and 1), each of the items listed by print_device_map, including all layers, lm_head, and norm, should have the values 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5836ff-0ed6-4875-93ca-5357756e4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_map(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774e769-d0ec-43bb-8d16-743522ddd170",
   "metadata": {},
   "source": [
    "We'll now clear the quantized model from memory so that we can re-load the locally stored version, for confirming that complete process was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667d612-9a8f-46bb-92df-11bc78f0b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db34c6-217b-4057-a323-f2ad30f0b848",
   "metadata": {},
   "source": [
    "Ensure that the GPU device(s) memory have been freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd2010-c4f8-496d-910f-b3ffcf87334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c38b96-55a8-41a4-9943-ed2d47ac5067",
   "metadata": {},
   "source": [
    "Now we'll local the locally stored, quantized model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496ca1a-d0dd-4694-94d0-c35f1d9e6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    quantized_model_dir,\n",
    "    device_map=\"sequential\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0a709-d00f-4dee-8813-ee9c91a64c42",
   "metadata": {},
   "source": [
    "We should see that the locally stored model has the same quantization and memory usage as the model that was quantized as it was loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833dfad-60fd-4b90-a571-3aac2aa6a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_map(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
