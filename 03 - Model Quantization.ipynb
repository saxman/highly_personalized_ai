{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e511f719-52de-4c76-91eb-d73b99e6060f",
   "metadata": {},
   "source": [
    "# Model Quantization #\n",
    "\n",
    "Depending on the size of the model and how much GPU memory is available for running an LLM, the model may need to be quantized to ensure that it can fit in GPU memory. As a rough calculation, LLMs need 4x the number of parameters that it has, for memory. This is because each parameter is typically stored in a 32-bit (4 Byte) float. For example, an 7-billion (\"7B\") parameter model (e.g. Mistral 7B) requires around 28 GB or memory to run.\n",
    "\n",
    "Model quantization reduces the amount of memory required by each model parameter. For example, 8-bit quantization reduces the amount of memory required for a 7-billion parameter model to 7 GB, since each parameter uses one byte instead of four. 4-bit quantization further reduces the amount of memory required to around 3.5 GB.\n",
    "\n",
    "It's generally better to use a larger (in terms of parameters), quantized model rather than a smaller, un-quantized model of the same genre, as long as your quantization is >= 2 bits per parameter. The following article shows the drop-off in quality of model responses as fewer bits are used per model parameter, for the Llama 3 8B and 70B models.\n",
    "\n",
    "https://github.com/matt-c1/llama-3-quant-comparison\n",
    "\n",
    "In this notebook, we download full-sized models (no quantization) from HuggingFace, apply 4-bit or 8-bit quantization via the Bitsandbytes library, and store the qunatized model for later use. This speeds up model loading time since the quantized models don't need to be quantized at load-time.\n",
    "\n",
    "References:\n",
    "https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996\n",
    "https://towardsai.net/p/artificial-intelligence/llm-quantization-quantize-model-with-gptq-awq-and-bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769c44f-2f60-45ce-be2a-24d6a4dffdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e460f29-d0f8-4d78-9cd8-16d486a2a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade transformers\n",
    "!pip install --quiet --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64f347-fdab-4bf6-94a9-70753db289e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f31de1f-e4b1-4076-89f8-f80010353404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "model_id = \"google/gemma-2-27b-it\"\n",
    "\n",
    "## 8-bit quantization\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "## 4-bit quantization\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config_8bit,\n",
    "    # quantization_config=bnb_config_4bit,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"sequential\" ## using sequential instead of auto/balanced since otherwise lm_head can get put on CPU\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81964122-3679-4692-9b4a-e274fbc79ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"models/\" + model_id.replace(\"/\", \"__\")\n",
    "\n",
    "model.save_pretrained(quantized_model_dir)\n",
    "_ = tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a0462-79b2-4688-9fdf-3a894d54f54b",
   "metadata": {},
   "source": [
    "If the model was reduced to a size that will fit into GPU memory, through quantization, when we print the model device map, the items listed should all map the GPU device (or one of the devices) listed by print_device_info. None of the layers should map to CPU. For example, if we have two GPU devices available and listed (0 and 1), each of the items listed by print_device_map, including all layers, lm_head, and norm, should have the values 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5836ff-0ed6-4875-93ca-5357756e4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_map(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774e769-d0ec-43bb-8d16-743522ddd170",
   "metadata": {},
   "source": [
    "We'll now clear the quantized model from memory so that we can re-load the locally stored version, for confirming that complete process was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667d612-9a8f-46bb-92df-11bc78f0b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db34c6-217b-4057-a323-f2ad30f0b848",
   "metadata": {},
   "source": [
    "Ensure that the GPU device(s) memory have been freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd2010-c4f8-496d-910f-b3ffcf87334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c38b96-55a8-41a4-9943-ed2d47ac5067",
   "metadata": {},
   "source": [
    "Now we'll local the locally stored, quantized model into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496ca1a-d0dd-4694-94d0-c35f1d9e6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    quantized_model_dir,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    quantized_model_dir,\n",
    "    device_map=\"sequential\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0a709-d00f-4dee-8813-ee9c91a64c42",
   "metadata": {},
   "source": [
    "We should see that the locally stored model has the same quantization and memory usage as the model that was quantized as it was loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0833dfad-60fd-4b90-a571-3aac2aa6a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.print_device_info()\n",
    "utils.print_model_info(model)\n",
    "utils.print_device_map(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
