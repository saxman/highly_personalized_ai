{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63b14de-21d0-4145-a2da-d2da2af06ff5",
   "metadata": {},
   "source": [
    "# Knowledge Extraction from Chat Conversations #\n",
    "\n",
    "This notebook explores several ways that knowledge about an individual can be extracted from a chat transcript. This knowledge is stored in different formats, including descriptive summaries, JSON lists, etc. A subsequent notebook explores which format is best for retrieving information about the user in order to simulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677e2657-41e1-4542-a804-150d7c8fa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8287d5-bbb5-4ae4-bf7a-19dd2f7e40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --upgrade transformers\n",
    "!pip install --quiet --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9987e568-9e72-46b1-9763-658dd859f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bbb39e2-0419-48dc-8e19-43246698b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a3c732-102c-40e5-8640-91cfaf09ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b64e306-c01f-4509-ac81-a8cbd7bb4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"assistant\", \"content\": \"what is your name?\"},\n",
    "    {\"role\": \"user\", \"content\": \"my name is John\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"how are you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm great, thanks\" },\n",
    "    {\"role\": \"assistant\", \"content\": \"are you a student or employed?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I am a farmer\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"are you seeing someone or are you married?\"},\n",
    "    {\"role\": \"user\", \"content\": \"yes, my wife's name is Jane\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"how long have you been married?\"},\n",
    "    {\"role\": \"user\", \"content\": \"15 years\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"do you have any children?\"},\n",
    "    {\"role\": \"user\", \"content\": \"yes, two\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"what are their names?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Jack and Jill\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"how old are they?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10 and 12, respectively\"},\n",
    "]\n",
    "\n",
    "conversation_str = \"\"\n",
    "for message in conversation:\n",
    "    conversation_str += f\"{message['role']}: {message['content']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a187278-6cc8-4020-98e4-35f2ad0122f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "## Quantized Gemma model, stored locally\n",
    "# model_id = \"models/google__gemma-2-27b-it\"\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "## Quantized Llama model, stored locally\n",
    "# model_id = \"models/meta-llama__Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"low_cpu_mem_usage\": True,\n",
    "    \"device_map\": \"sequential\", # load the model into GPUs sequentially, to avoid memory allocation issues with balancing\n",
    "    \"torch_dtype\": \"auto\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b2105e-4b2d-45cc-b2c6-c18fc9bdc948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eb13787832477bb984224a3dcca111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2f31196e0a43439ec784fe89d2a819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e3987a075f467ebe8df05fe36ebbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    **model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28b0149-befe-4567-86a2-719d35c9ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : size : 15316 MB\n",
      "model : is quantized : False\n",
      "model : on GPU : True\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.print_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e9f0fd-76e2-45ae-865a-8a262749f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"bos_token_id\": tokenizer.bos_token_id,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "if 'llama' in model_id.lower():\n",
    "    generate_kwargs[\"eos_token_id\"] = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "else:\n",
    "    generate_kwargs[\"eos_token_id\"] = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e9afd0d-7661-498a-bee3-7919598d2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a paragraph summary of the facts about the user, John:\n",
      "\n",
      "John is a 15-year married farmer who resides with his wife, Jane, and their two children, Jack and Jill. He has been married to Jane for 15 years and has two children, a 10-year-old son named Jack and a 12-year-old daughter named Jill.\n",
      "\n",
      "CPU times: user 1.83 s, sys: 239 ms, total: 2.07 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"\n",
    "Create a paragraph summary of facts about the user from the following coversation:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt + conversation_str}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    **generate_kwargs\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "print(tokenizer.decode(response, skip_special_tokens=True) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e20b5c-3ff7-49ce-9db5-8c4fe13f487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    # model=model_id,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    tokenizer=tokenizer,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cba27a4-462c-4840-9b3e-a40886f6fb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the conversation, here is a paragraph summary of facts about John:\n",
      "\n",
      "John is a 15-year married farmer who lives with his wife, Jane. He has two children, Jack and Jill, who are 10 and 12 years old, respectively. John is currently in great spirits and enjoys his life as a farmer.<|eot_id|>\n",
      "CPU times: user 1.38 s, sys: 4.17 ms, total: 1.38 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"\n",
    "Create a paragraph summary of facts about the user from the following coversation:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt + conversation_str}\n",
    "]\n",
    "\n",
    "_ = pipe(\n",
    "    messages,\n",
    "    **generate_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a95137c8-447f-4e95-966c-8a4a0a196ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided chat conversation, the following are the discrete facts about the user:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"fact\": \"The user's name is John\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user is a farmer\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user is married\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user's wife's name is Jane\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user has been married for 15 years\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user has two children\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user's children's names are Jack and Jill\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user's children are 10 and 12 years old\",\n",
      "    \"confidence_score\": 1.0\n",
      "  }\n",
      "]\n",
      "```<|eot_id|>\n",
      "CPU times: user 4.92 s, sys: 18.5 ms, total: 4.94 s\n",
      "Wall time: 4.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an AI assistant that lists facts about the user from chat conversations.\n",
    "You list facts in a JSON array, with each fact in a dictionary with keys fact and confidence_score.\n",
    "List all discrete facts about the user from the following chat conversation:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt + conversation_str}\n",
    "]\n",
    "\n",
    "_ = pipe(\n",
    "    messages,\n",
    "    **generate_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "785a9fff-9f74-4243-bf63-de7526132f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "[\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"has the name\",\n",
      "    \"object\": \"John\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"is great\",\n",
      "    \"object\": \"emotion\",\n",
      "    \"confidence_score\": 0.9\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"is a farmer\",\n",
      "    \"object\": \"profession\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"is married\",\n",
      "    \"object\": \"status\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"has a wife named\",\n",
      "    \"object\": \"Jane\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"has been married for\",\n",
      "    \"object\": \"15 years\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"has two children\",\n",
      "    \"object\": \"number of children\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"John\",\n",
      "    \"predicate\": \"has children named\",\n",
      "    \"object\": \"Jack and Jill\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack and Jill\",\n",
      "    \"predicate\": \"are 10 and 12 years old\",\n",
      "    \"object\": \"age\",\n",
      "    \"confidence_score\": 1.0\n",
      "  }\n",
      "]\n",
      "```<|eot_id|>\n",
      "CPU times: user 7.1 s, sys: 34.9 ms, total: 7.14 s\n",
      "Wall time: 7.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an AI assistant that identifies facts about the user from chat conversations.\n",
    "You list facts in a JSON array, with each fact in a dictionary with keys subject, predicate, object, and confidence_score.\n",
    "List all discrete facts about the user from the following chat conversation:\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt + conversation_str}\n",
    "]\n",
    "\n",
    "_ = pipe(\n",
    "    messages,\n",
    "    **generate_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac0540-2c0a-469e-82e8-902e4891494c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
