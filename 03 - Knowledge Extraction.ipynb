{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e2657-41e1-4542-a804-150d7c8fa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8287d5-bbb5-4ae4-bf7a-19dd2f7e40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987e568-9e72-46b1-9763-658dd859f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb39e2-0419-48dc-8e19-43246698b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3c732-102c-40e5-8640-91cfaf09ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a187278-6cc8-4020-98e4-35f2ad0122f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"GritLM/GritLM-7B\"\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# model_name = \"google/gemma-1.1-7b-it\"\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "# model_name = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "## 4-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "## 8-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=False\n",
    "# )\n",
    "\n",
    "model_kwargs = {\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    # \"torch_dtype\": \"auto\",\n",
    "    # \"quantization_config\": bnb_config,\n",
    "}\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e20b5c-3ff7-49ce-9db5-8c4fe13f487c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3204e38ceabc4c5ebf0d653d478a8427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    "    device=device,\n",
    "    streamer=streamer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28b0149-befe-4567-86a2-719d35c9ee82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size : 15316 MB\n",
      "Model is quantized : False\n",
      "Model on GPU : True\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "utils.print_model_info(pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b64e306-c01f-4509-ac81-a8cbd7bb4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"assistant\", \"content\": \"what is your name?\"},\n",
    "    {\"role\": \"user\", \"content\": \"my name is Jack\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"how are you today, Jack?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm great, thanks\" },\n",
    "    {\"role\": \"assistant\", \"content\": \"are you a student or employed?\"},\n",
    "    {\"role\": \"user\", \"content\": \"I am a farmer\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"are you seeing someone or are you married?\"},\n",
    "    {\"role\": \"user\", \"content\": \"yes, my wife's name is Jill\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"how long have you been married?\"},\n",
    "    {\"role\": \"user\", \"content\": \"15 years\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"do you have any children?\"},\n",
    "    {\"role\": \"user\", \"content\": \"yes, two\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"what are their names?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Jack and Jill\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"how old are they?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10 and 12, respectively\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cba27a4-462c-4840-9b3e-a40886f6fb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Summarize information about the user from the following coversation:\n",
      "assistant: what is your name?\n",
      "user: my name is Jack\n",
      "assistant: how are you today, Jack?\n",
      "user: I'm great, thanks\n",
      "assistant: are you a student or employed?\n",
      "user: I am a farmer\n",
      "assistant: are you seeing someone or are you married?\n",
      "user: yes, my wife's name is Jill\n",
      "assistant: how long have you been married?\n",
      "user: 15 years\n",
      "assistant: do you have any children?\n",
      "user: yes, two\n",
      "assistant: what are their names?\n",
      "user: Jack and Jill\n",
      "assistant: how old are they?\n",
      "user: 10 and 12, respectively<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is a summary of the information about the user, Jack:\n",
      "\n",
      "* Name: Jack\n",
      "* Occupation: Farmer\n",
      "* Marital status: Married to Jill\n",
      "* Length of marriage: 15 years\n",
      "* Children: Two, named Jack and Jill\n",
      "* Ages of children: 10 and 12, respectively<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Summarize information about the user from the following coversation:\n",
    "\"\"\"\n",
    "\n",
    "for message in conversation:\n",
    "    prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "sequences = pipe(\n",
    "    messages,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.1,\n",
    "    num_return_sequences=1,\n",
    "    # eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95137c8-447f-4e95-966c-8a4a0a196ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are an AI assistant that identifies facts about the user from chat conversations.\n",
      "You list facts in a JSON array, with each fact in a dictionary with keys fact and confidence_score.\n",
      "The confidence score must be accurate.\n",
      "List all discrete facts about the user from the following chat conversation:\n",
      "assistant: what is your name?\n",
      "user: my name is Jack\n",
      "assistant: how are you today, Jack?\n",
      "user: I'm great, thanks\n",
      "assistant: are you a student or employed?\n",
      "user: I am a farmer\n",
      "assistant: are you seeing someone or are you married?\n",
      "user: yes, my wife's name is Jill\n",
      "assistant: how long have you been married?\n",
      "user: 15 years\n",
      "assistant: do you have any children?\n",
      "user: yes, two\n",
      "assistant: what are their names?\n",
      "user: Jack and Jill\n",
      "assistant: how old are they?\n",
      "user: 10 and 12, respectively<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here are the discrete facts about the user from the chat conversation:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"fact\": \"The user's name is Jack\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user is a farmer\",\n",
      "    \"confidence_score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user is married\",\n",
      "    \"confidence_score\": 0.9\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user's wife's name is Jill\",\n",
      "    \"confidence_score\": 0.9\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user has been married for 15 years\",\n",
      "    \"confidence_score\": 0.9\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user has two children\",\n",
      "    \"confidence_score\": 0.9\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user's children's names are Jack and Jill\",\n",
      "    \"confidence_score\": 0.8\n",
      "  },\n",
      "  {\n",
      "    \"fact\": \"The user's children are 10 and 12 years old\",\n",
      "    \"confidence_score\": 0.8\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note: The confidence scores are based on the accuracy of the user's responses and the likelihood of the facts being true. The scores are subjective and may vary depending on the context and the AI's understanding of the conversation.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant that identifies facts about the user from chat conversations.\n",
    "You list facts in a JSON array, with each fact in a dictionary with keys fact and confidence_score.\n",
    "The confidence score must be accurate.\n",
    "List all discrete facts about the user from the following chat conversation:\n",
    "\"\"\"\n",
    "\n",
    "for message in conversation:\n",
    "    prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "sequences = pipe(\n",
    "    messages,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "785a9fff-9f74-4243-bf63-de7526132f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "You are an AI assistant that identifies facts about the user from chat conversations.\n",
      "You list facts in a JSON array, with each fact in a dictionary with keys subject, predicate, object, and confidence_score.\n",
      "The confidence score must be accurate.\n",
      "List all discrete facts about the user from the following chat conversation:\n",
      "assistant: what is your name?\n",
      "user: my name is Jack\n",
      "assistant: how are you today, Jack?\n",
      "user: I'm great, thanks\n",
      "assistant: are you a student or employed?\n",
      "user: I am a farmer\n",
      "assistant: are you seeing someone or are you married?\n",
      "user: yes, my wife's name is Jill\n",
      "assistant: how long have you been married?\n",
      "user: 15 years\n",
      "assistant: do you have any children?\n",
      "user: yes, two\n",
      "assistant: what are their names?\n",
      "user: Jack and Jill\n",
      "assistant: how old are they?\n",
      "user: 10 and 12, respectively<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here are the discrete facts about the user from the chat conversation:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"has_name\",\n",
      "    \"object\": \"Jack\",\n",
      "    \"confidence_score\": 0.95\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"is\",\n",
      "    \"object\": \"great\",\n",
      "    \"confidence_score\": 0.85\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"is_a\",\n",
      "    \"object\": \"farmer\",\n",
      "    \"confidence_score\": 0.90\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"is_married_to\",\n",
      "    \"object\": \"Jill\",\n",
      "    \"confidence_score\": 0.95\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"has_spouse\",\n",
      "    \"object\": \"Jill\",\n",
      "    \"confidence_score\": 0.95\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"has_married_for\",\n",
      "    \"object\": \"15 years\",\n",
      "    \"confidence_score\": 0.90\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"has_children\",\n",
      "    \"object\": \"2\",\n",
      "    \"confidence_score\": 0.95\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"has_child\",\n",
      "    \"object\": \"Jack\",\n",
      "    \"confidence_score\": 0.95\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"has_child\",\n",
      "    \"object\": \"Jill\",\n",
      "    \"confidence_score\": 0.95\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"child_age\",\n",
      "    \"object\": \"10\",\n",
      "    \"confidence_score\": 0.90\n",
      "  },\n",
      "  {\n",
      "    \"subject\": \"Jack\",\n",
      "    \"predicate\": \"child_age\",\n",
      "    \"object\": \"12\",\n",
      "    \"confidence_score\": 0.90\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "Note: The confidence scores are subjective and may vary based on the complexity of the conversation and the AI's ability to understand the user's responses.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are an AI assistant that identifies facts about the user from chat conversations.\n",
    "You list facts in a JSON array, with each fact in a dictionary with keys subject, predicate, object, and confidence_score.\n",
    "The confidence score must be accurate.\n",
    "List all discrete facts about the user from the following chat conversation:\n",
    "\"\"\"\n",
    "\n",
    "for message in conversation:\n",
    "    prompt += f\"{message['role']}: {message['content']}\\n\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "sequences = pipe(\n",
    "    messages,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae240a3-c8ab-40e6-b854-25a6ff90a9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
